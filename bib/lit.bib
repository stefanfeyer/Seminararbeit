@inproceedings{Anderson2013a,
address = {New York, NY, USA},
author = {Anderson, Fraser and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
booktitle = {Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology},
doi = {10.1145/2501988.2502045},
isbn = {978-1-4503-2268-3},
keywords = {3d,augmented reality,full body,guidance.,learning,motor learning,movement,training},
pages = {311--320},
publisher = {ACM},
series = {UIST '13},
title = {{YouMove: Enhancing Movement Training with an Augmented Reality Mirror}},
url = {http://doi.acm.org/10.1145/2501988.2502045},
year = {2013}
}
@article{Chan2011a,
author = {Chan, J C P and Leung, H and Tang, J K T and Komura, T},
doi = {10.1109/TLT.2010.27},
journal = {IEEE Transactions on Learning Technologies},
keywords = {Animation,Color,Games,Joints,Rendering (computer graphics),Three dimensional displays,Training,computer aided instruction,computer uses in education,humanities,motion analysis.,motion capture technology,movement learning,virtual reality,virtual reality dance training system,virtual teacher},
month = {apr},
number = {2},
pages = {187--195},
title = {{A Virtual Reality Dance Training System Using Motion Capture Technology}},
volume = {4},
year = {2011}
}
@inproceedings{Chan2007,
address = {ICST, Brussels, Belgium, Belgium},
author = {Chan, Jacky and Leung, Howard and Tang, Kai Tai and Komura, Taku},
booktitle = {Proceedings of the First International Conference on Immersive Telecommunications},
isbn = {978-963-9799-06-6},
keywords = {3d human motion analysis,human computer interaction,immersive VR application,performance training},
pages = {7:1----7:6},
publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
series = {ImmersCom '07},
title = {{Immersive Performance Training Tools Using Motion Capture Technology}},
url = {http://dl.acm.org/citation.cfm?id=1538981.1538991},
year = {2007}
}
@article{Chinthammit2014,
author = {Chinthammit, Winyu and Merritt, Troy and Scott, Pedersen and Andrew, Williams and Denis, Visentin and Rowe, Robert and Thomas, Furness},
journal = {BioMed Research International},
title = {{Ghostman: Augmented Reality Application for Telerehabilitation and Remote Instruction of a Novel Motor Skill}},
volume = {2014},
year = {2014}
}
@article{Choensawat2015,
address = {Hingham, MA, USA},
author = {Choensawat, Worawat and Nakamura, Minako and Hachimura, Kozaburo},
doi = {10.1007/s11042-014-2209-6},
issn = {1380-7501},
journal = {Multimedia Tools Appl.},
keywords = {Dance notation,LabanEditor,Labanotation data,Movement aAnalysis},
month = {dec},
number = {23},
pages = {10823--10846},
publisher = {Kluwer Academic Publishers},
title = {{GenLaban: A Tool for Generating Labanotation from Motion Capture Data}},
url = {http://dx.doi.org/10.1007/s11042-014-2209-6},
volume = {74},
year = {2015}
}
@inproceedings{Covaci2014,
address = {New York, NY, USA},
author = {Covaci, Alexandra and Olivier, Anne-H{\'{e}}l{\`{e}}ne and Multon, Franck},
booktitle = {Proceedings of the 20th ACM Symposium on Virtual Reality Software and Technology},
doi = {10.1145/2671015.2671023},
isbn = {978-1-4503-3253-8},
keywords = {basketball training,immersive room,perception of distance in VR,performance,visual feedback},
pages = {55--64},
publisher = {ACM},
series = {VRST '14},
title = {{Third Person View and Guidance for More Natural Motor Behaviour in Immersive Basketball Playing}},
url = {http://doi.acm.org/10.1145/2671015.2671023},
year = {2014}
}
@inproceedings{Hachimura2004,
author = {Hachimura, K and Kato, H and Tamura, H},
booktitle = {RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)},
doi = {10.1109/ROMAN.2004.1374759},
keywords = {Character generation,Cultural differences,Education,Facial animation,Humans,Layout,Motion measurement,Motion pictures,Prototypes,Virtual reality,computer based training,computer graphics,dance education,humanities,interactive systems,mixed reality technology,motion capture technology,prototype dance training support system,prototypes,real world scenes,user interface functions,user interfaces,virtual reality,virtual world scenes},
pages = {217--222},
title = {{A prototype dance training support system with motion capture and mixed reality technologies}},
year = {2004}
}
@inproceedings{Han2016,
address = {New York, NY, USA},
author = {Han, Ping-Hsuan and Chen, Kuan-Wen and Hsieh, Chen-Hsin and Huang, Yu-Jie and Hung, Yi-Ping},
booktitle = {Proceedings of the 7th Augmented Human International Conference 2016},
doi = {10.1145/2875194.2875237},
isbn = {978-1-4503-3680-2},
keywords = {Augmented Reality,Body Movement Guidance,Visualization,Wearable Interaction},
pages = {31:1----31:4},
publisher = {ACM},
series = {AH '16},
title = {{AR-Arm: Augmented Visualization for Guiding Arm Movement in the First-Person Perspective}},
url = {http://doi.acm.org/10.1145/2875194.2875237},
year = {2016}
}
@inproceedings{Han2017,
address = {New York, NY, USA},
author = {Han, Ping-Hsuan and Chen, Yang-Sheng and Zhong, Yilun and Wang, Han-Lei and Hung, Yi-Ping},
booktitle = {Proceedings of the 8th Augmented Human International Conference},
doi = {10.1145/3041164.3041194},
isbn = {978-1-4503-4835-5},
keywords = {Tai-Chi Chuan,augmented mirror,drone,mixed reality,physical activity learning},
pages = {25:1----25:4},
publisher = {ACM},
series = {AH '17},
title = {{My Tai-Chi Coaches: An Augmented-learning Tool for Practicing Tai-Chi Chuan}},
url = {http://doi.acm.org/10.1145/3041164.3041194},
year = {2017}
}
@inproceedings{Hoang2016,
address = {New York, NY, USA},
author = {Hoang, Thuong N and Reinoso, Martin and Vetere, Frank and Tanin, Egemen},
booktitle = {Proceedings of the 9th Nordic Conference on Human-Computer Interaction},
doi = {10.1145/2971485.2971521},
isbn = {978-1-4503-4763-1},
keywords = {Virtual reality,first person view,posture guidance},
pages = {25:1----25:10},
publisher = {ACM},
series = {NordiCHI '16},
title = {{Onebody: Remote Posture Guidance System Using First Person View in Virtual Environment}},
url = {http://doi.acm.org/10.1145/2971485.2971521},
year = {2016}
}
@inproceedings{Katzakis2017,
address = {New York, NY, USA},
author = {Katzakis, Nicholas and Tong, Jonathan and Ariza, Oscar and Chen, Lihan and Klinker, Gudrun and R{\"{o}}der, Brigitte and Steinicke, Frank},
booktitle = {Proceedings of the 5th Symposium on Spatial User Interaction},
doi = {10.1145/3131277.3132181},
isbn = {978-1-4503-5486-8},
keywords = {haptic,integration,interface,multisensory,perception,posture training,sensory,visual,wearable},
pages = {58--67},
publisher = {ACM},
series = {SUI '17},
title = {{Stylo and Handifact: Modulating Haptic Perception Through Visualizations for Posture Training in Augmented Reality}},
url = {http://doi.acm.org/10.1145/3131277.3132181},
year = {2017}
}
@inproceedings{Kojima2014,
abstract = {The basic of training physical skills is to imitate instructor's motion. Observation is the very first step to copy the motion of instructor when at the beginning of learning sports of artisanship. However, beginners face difficulties in imitating at the start since they do not have somesthetic image of the movement. In order to help learning physical skills, we propose Immersive virtual environment using head mounted display that indicates 3D motion of instructor super imposed on learner's body. By using this system, learners try to match its own form to instructor's 3D model to imitate instructor's motion from first person view in virtual environment. At the early stage of this research, we tried to transfer pitching skill in baseball. We evaluated the effectiveness of proposed system by measuring throwing distance.},
address = {Cham},
author = {Kojima, Taihei and Hiyama, Atsushi and Miura, Takahiro and Hirose, Michitaka},
booktitle = {Human Interface and the Management of Information. Information and Knowledge in Applications and Services},
editor = {Yamamoto, Sakae},
isbn = {978-3-319-07863-2},
pages = {51--58},
publisher = {Springer International Publishing},
title = {{Training Archived Physical Skill through Immersive Virtual Environment}},
year = {2014}
}
@misc{LaViola2017,
address = {Boston },
annote = {QA76.9.U83},
author = {LaViola, Joseph J},
edition = {Second },
isbn = {0134034325;9780134034478;9780134034324;0134034473;},
keywords = {Three-dimensional display systems,User interfaces (Computer systems),Virtual reality},
publisher = {Addison-Wesley },
title = {{3D user interfaces: theory and practice }},
url = {http://konstanz.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwfV3LisMgFL102s3AQF8zzPRF-gEN0WsSXfdBobvSrouJZtlFH{\_}9fr00mtNC6E0VQ0HM8HI8AyMNo9nQmcM1ytMxggTlqmWQsNo6oSpsUKpEmfjJfVEpGdTqGFGgannXh7o9EPs{\_}3GAlyYVCCDkPhk-vkB7QcIqVk7Vts97XWogQZ1n2iju-KPK4rIsUyia},
year = {2017}
}
@article{Lieberman2007,
author = {Lieberman, J and Breazeal, C},
doi = {10.1109/TRO.2007.907481},
journal = {IEEE Transactions on Robotics},
keywords = {5-DOF robotic suit,Acceleration,Education,Educational robots,Human factors,Humans,Intelligent robots,Intelligent systems,Joints,Learning systems,Optical feedback,Real time systems,TIKL,auditory feedback,feedback,haptic interfaces,human motor learning,intelligent tutoring systems,kinesthetic learning,man–machine systems,motor rehabilitation,real-time systems,realtime tactile feedback,robot tactile systems,robots,sports training,visual feedback,wearable computers,wearable robotic system,wearable vibrotactile feedback suit},
month = {oct},
number = {5},
pages = {919--926},
title = {{TIKL: Development of a Wearable Vibrotactile Feedback Suit for Improved Human Motor Learning}},
volume = {23},
year = {2007}
}
@article{Milgram1994,
author = {Milgram, Paul and Kishino, Fumio},
journal = {IEICE Trans. Information Systems},
pages = {1321--1329},
title = {{A Taxonomy of Mixed Reality Visual Displays}},
volume = {vol. E77-D},
year = {1994}
}
@inproceedings{Chua2003,
author = {{Philo Tan Chua} and Crivella, R and Daly, B and {Ning Hu} and Schaaf, R and Ventura, D and Camill, T and Hodgins, J and Pausch, R},
booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
doi = {10.1109/VR.2003.1191125},
keywords = {Animation,Displays,Optical feedback,Testing,Tracking,User interfaces,Virtual environment,Virtual prototyping,Virtual reality,Wires,animated representation,belt-worn video receiver,computer based training,full body Tai Chi training application,head-mounted display,helmet mounted displays,immersive techniques,mobile computing,motion training tasks,physical task training,sport,user interface,user interfaces,virtual environments,virtual reality,virtual teacher,wireless virtual reality system},
month = {mar},
pages = {87--94},
title = {{Training for physical tasks in virtual environments: Tai Chi}},
year = {2003}
}
@inproceedings{Portillo2008,
abstract = {This paper presents a multimodal system capable to understand and correct in real-time the movements of Tai-Chi students through the integration of audio-visual-tactile technologies. This platform acts like a virtual teacher that transfers the knowledge of five Tai-Chi movements using feed-back stimuli to compensate the errors committed by a user during the performance of the gesture. The fundamental components of this multimodal interface are the gesture recognition system (using k-means clustering, Probabilistic Neural Networks (PNN) and Finite State Machines (FSM)) and the real-time descriptor of motion which is used to compute and qualify the actual movements performed by the student respect to the movements performed by the master, obtaining several feedbacks and compensating this movement in real-time varying audio-visualtactile parameters of different devices. The experiments of this multimodal platform have confirmed that the quality of the movements performed by the students is improved significantly.},
address = {Berlin, Heidelberg},
author = {Portillo-Rodriguez, Otniel and Sandoval-Gonzalez, Oscar O and Ruffaldi, Emanuele and Leonardi, Rosario and Avizzano, Carlo Alberto and Bergamasco, Massimo},
booktitle = {Haptic and Audio Interaction Design},
editor = {Pirhonen, Antti and Brewster, Stephen},
isbn = {978-3-540-87883-4},
pages = {30--39},
publisher = {Springer Berlin Heidelberg},
title = {{Real-Time Gesture Recognition, Evaluation and Feed-Forward Correction of a Multimodal Tai-Chi Platform}},
year = {2008}
}
@article{Rajanna2015,
abstract = {A carefully planned, structured, and supervised physiotherapy program, following a surgery, is crucial for the successful diagnosis of physical injuries. Nearly 50 {\%} of the surgeries fail due to unsupervised, and erroneous physiotherapy. The demand for a physiotherapist for an extended period is expensive to afford, and sometimes inaccessible. Researchers have tried to leverage the advancements in wearable sensors and motion tracking by building affordable, automated, physio-therapeutic systems that direct a physiotherapy session by providing audio-visual feedback on patient's performance. There are many aspects of automated physiotherapy program which are yet to be addressed by the existing systems: a wide classification of patients' physiological conditions to be diagnosed, multiple demographics of the patients (blind, deaf, etc.), and the need to pursue patients to adopt the system for an extended period for self-care. In our research, we have tried to address these aspects by building a health behavior change support system called KinoHaptics, for post-surgery rehabilitation. KinoHaptics is an automated, wearable, haptic assisted, physio-therapeutic system that can be used by a wide variety of demographics and for various physiological conditions of the patients. The system provides rich and accurate vibro-haptic feedback that can be felt by the user, irrespective of the physiological limitations. KinoHaptics is built to ensure that no injuries are induced during the rehabilitation period. The persuasive nature of the system allows for personal goal-setting, progress tracking, and most importantly life-style compatibility. The system was evaluated under laboratory conditions, involving 14 users. Results show that KinoHaptics is highly convenient to use, and the vibro-haptic feedback is intuitive, accurate, and has shown to prevent accidental injuries. Also, results show that KinoHaptics is persuasive in nature as it supports behavior change and habit building. The successful acceptance of KinoHaptics, an automated, wearable, haptic assisted, physio-therapeutic system proves the need and future-scope of automated physio-therapeutic systems for self-care and behavior change. It also proves that such systems incorporated with vibro-haptic feedback encourage strong adherence to the physiotherapy program; can have profound impact on the physiotherapy experience resulting in higher acceptance rate.},
author = {Rajanna, Vijay and Vo, Patrick and Barth, Jerry and Mjelde, Matthew and Grey, Trevor and Oduola, Cassandra and Hammond, Tracy},
doi = {10.1007/s10916-015-0391-3},
issn = {1573-689X},
journal = {Journal of Medical Systems},
month = {dec},
number = {3},
pages = {60},
title = {{KinoHaptics: An Automated, Wearable, Haptic Assisted, Physio-therapeutic System for Post-surgery Rehabilitation and Self-care}},
url = {https://doi.org/10.1007/s10916-015-0391-3},
volume = {40},
year = {2015}
}
@inproceedings{Qian2005,
abstract = {We present a hybrid classification method applicable to gesture recognition. The method combines elements of Hidden Markov Models (HMM) and various Dynamic Programming Alignment (DPA) methods, such as edit distance, sequence alignment, and dynamic time warping. As opposed to existing approaches which treat HMM and DPA as either competing or complementing methods, we provide a common framework which allows us to combine ideas from both HMM and DPA research. The combined approach takes on the robustness and effectiveness of HMMs and the simplicity of DPA approaches. We have implemented and successfully tested the proposed algorithm on various gesture data.},
address = {Berlin, Heidelberg},
author = {Rajko, Stjepan and Qian, Gang},
booktitle = {Advances in Visual Computing},
editor = {Bebis, George and Boyle, Richard and Koracin, Darko and Parvin, Bahram},
isbn = {978-3-540-32284-9},
pages = {227--234},
publisher = {Springer Berlin Heidelberg},
title = {{A Hybrid HMM/DPA Adaptive Gesture Recognition Method}},
year = {2005}
}
@inproceedings{Scavo2015,
author = {Scavo, Giuseppe and Wild, Fridolin and Scott, Peter},
booktitle = {Workshop Proceedings of the 11th International Conference on Intelligent Environments},
pages = {236--243},
title = {{No Title}},
year = {2015}
}
@book{Schmidt2011,
author = {Schmidt, Richard A. and Lee, Tim},
edition = {5th Editio},
isbn = {978-0736079617},
publisher = {Human Kinetics},
title = {{Motor Control and Learning}},
year = {2011}
}
@inproceedings{Sodhi2012,
address = {New York, NY, USA},
author = {Sodhi, Rajinder and Benko, Hrvoje and Wilson, Andrew},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2207676.2207702},
isbn = {978-1-4503-1015-4},
keywords = {appropriated surfaces,on-body computing,on-demand interfaces,spatial augmented reality,tracking},
pages = {179--188},
publisher = {ACM},
series = {CHI '12},
title = {{LightGuide: Projected Visualizations for Hand Movement Guidance}},
url = {http://doi.acm.org/10.1145/2207676.2207702},
year = {2012}
}
@inproceedings{Sousa2016,
address = {New York, NY, USA},
author = {Sousa, Maur$\backslash$'$\backslash$icio and Vieira, Jo{\~{a}}o and Medeiros, Daniel and Arsenio, Artur and Jorge, Joaquim},
booktitle = {Proceedings of the 21st International Conference on Intelligent User Interfaces},
doi = {10.1145/2856767.2856773},
isbn = {978-1-4503-4137-0},
keywords = {augmented reality,projection-based systems,rehabilitation},
pages = {175--185},
publisher = {ACM},
series = {IUI '16},
title = {{SleeveAR: Augmented Reality for Rehabilitation Using Realtime Feedback}},
url = {http://doi.acm.org/10.1145/2856767.2856773},
year = {2016}
}
@inproceedings{Tang2015,
address = {New York, NY, USA},
author = {Tang, Richard and Yang, Xing-Dong and Bateman, Scott and Jorge, Joaquim and Tang, Anthony},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
doi = {10.1145/2702123.2702401},
isbn = {978-1-4503-3145-6},
keywords = {augmented reality,movement guidance,physiotherapy,visualization},
pages = {4123--4132},
publisher = {ACM},
series = {CHI '15},
title = {{Physio@Home: Exploring Visual Guidance and Feedback Techniques for Physiotherapy Exercises}},
url = {http://doi.acm.org/10.1145/2702123.2702401},
year = {2015}
}
@inproceedings{Velloso2013,
address = {New York, NY, USA},
author = {Velloso, Eduardo and Bulling, Andreas and Gellersen, Hans},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/2470654.2466171},
isbn = {978-1-4503-1899-0},
keywords = {activity assessment,learning by demonstration,motion modelling,real-time user feedback,weight lifting},
pages = {1309--1318},
publisher = {ACM},
series = {CHI '13},
title = {{MotionMA: Motion Modelling and Analysis by Demonstration}},
url = {http://doi.acm.org/10.1145/2470654.2466171},
year = {2013}
}
@article{Wang2001,
abstract = {Dynamic viewpoint tethering is an innovative display technique which has been proposed to support effective navigation in large-scale virtual environments, by integrating information from different frames of reference. The present study examines the effect of dynamic viewpoint tethering on performance, with respect to both local guidance and global awareness measures, in comparison with three conventional display formats: egocentric, exocentric and rigidly tethered displays. Participants were instructed to control an aircraft-shaped cursor navigating in a virtual tunnel and to answer questions about the environment. The results confirmed that global awareness performance decreased with increased egocentricity in the display frame of reference. The two tethered displays (dynamic and rigid) supported the best local guidance performance. No significant performance differences were found between the two tethered displays.},
author = {Wang, Wenbi and Milgram, Paul},
doi = {10.1177/154193120104502702},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Milgram - 2001 - Dynamic Viewpoint Tethering for Navigation in Large-scale Virtual Environments.pdf:pdf},
issn = {1071-1813},
journal = {Human Factors},
keywords = {awareness,dynamically tethered displays,navigation,virtual cameras,virtual environments},
pages = {1862--1866},
title = {{Dynamic Viewpoint Tethering for Navigation in Large-scale Virtual Environments}},
year = {2001}
}
@inproceedings{Yan2015,
address = {New York, NY, USA},
author = {Yan, Shuo and Ding, Gangyi and Guan, Zheng and Sun, Ningxiao and Li, Hongsong and Zhang, Longfei},
booktitle = {Proceedings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Systems},
doi = {10.1145/2702613.2732759},
isbn = {978-1-4503-3146-3},
keywords = {augmented human,choreography,external self-image,mixed reality,visual blending},
pages = {965--970},
publisher = {ACM},
series = {CHI EA '15},
title = {{OutsideMe: Augmenting Dancer's External Self-Image by Using A Mixed Reality System}},
url = {http://doi.acm.org/10.1145/2702613.2732759},
year = {2015}
}
@article{Yang2002,
author = {Yang, Ungyeon and Kim, Gerard Jounghyn},
journal = {Presence},
number = {No. 3},
pages = {304--323},
title = {{Implementation and Evaluation of "Just Follow Me": An Immersive, VR-Based, Motion-Training System}},
volume = {11},
year = {2002}
}
@article{Yoshimura2006,
abstract = {Abstract In this research, the authors evaluate the degree to which dancers copy or follow the techniques of a master, or the degree of proficiency, by analyzing movements in traditional Japanese dance. The data used consist of three-dimensional time series for traditional Japanese dance movements acquired using optical motion capture system. In the authors' prior research, three moving coordinate systems which would move according to the translation and rotation of the body were used to extract the portion of the target movement. In this research, the authors consider a moving coordinate system which simultaneously takes into consideration translation, rotation, correction of orientation, and correction of waist tremble. In their prior research, the authors defined indices for movement stability and frequency characteristics as indices to quantitatively represent in an objective fashion the degree of proficiency of a dancer. Separate from this, in the current research the authors define an index with the spectrum component using a Gabor transform and an index for the amount of translation. The authors had a total of five people, a master from a particular dance school and four dance students of different genders and at different experience levels (all the master's students), perform dance experiments. The authors then extracted the target movements, measured the indices using the extraction results, and attempted to evaluate the degree of proficiency based on the proposed indices. Extraction was sufficiently precise, and the authors were able to confirm that the indices represent the differences appearing due to degree of proficiency and gender. {\textcopyright} 2005 Wiley Periodicals, Inc. Syst Comp Jpn, 37(1): 71–82, 2006; Published online in Wiley InterScience (www.interscience. wiley.com). DOI 10.1002/scj.20250},
author = {Yoshimura, Mitsu and Murasato, Hideki and Kai, Tamiko and Kuromiya, Akira and Yokoyama, Kiyoko and Hachimura, Kozaburo},
doi = {10.1002/scj.20250},
journal = {Systems and Computers in Japan},
keywords = {Gabor transform,algorithm to extract movement,degree of proficiency,traditional Japanese dance},
number = {1},
pages = {71--82},
title = {{Analysis of Japanese dance movements using motion capture system}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/scj.20250},
volume = {37},
year = {2006}
}
