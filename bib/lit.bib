@article{Ahmad2016,
author = {Ahmad, Akhlaq and Rahman, Mohamed Abdur and Anderson, Fraser and Grossman, Tovi and Matejka, Justin and Dehzangi, Omid and Zhao, Zheng},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/max und ullis chi paper ref.pdf:pdf},
title = {{New York, NY, USA, 3272–3276. DOI:}},
year = {2016}
}
@article{Milgram1994,
author = {Milgram, Paul and Kishino, Fumio},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/Literatur/milgram{\_}continuum.pdf:pdf},
journal = {IEICE Transactions on Information Systems},
number = {12},
pages = {1--15},
title = {{A TAXONOMY OF MIXED REALITY VISUAL DISPLAYS. IEICE Transactions on Information Systems, Vol E77-D, No.12}},
volume = {E77-D},
year = {1994}
}
@article{Hoang2016,
author = {Hoang, Thuong N and Reinoso, Martin and Vetere, Farnk and Tanin, Egemen},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/Onebody Remote Posture Guidance System using First Person View in Virtual Environment.pdf:pdf},
isbn = {9781450347},
title = {{Onebody : Remote Posture Guidance System using First Person View in Virtual Environment}},
year = {2016}
}
@article{Portillo-rodriguez2008,
author = {Portillo-rodriguez, Otniel and Sandoval-gonzalez, Oscar O and Ruffaldi, Emanuele and Leonardi, Rosario and Avizzano, Carlo Alberto and Bergamasco, Massimo},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/Real-Time Gesture Recognition, Evaluation and Feed-.pdf:pdf},
keywords = {audio-position feedback,multimodal interfaces,ognition,real-time 3d time-independent gesture,real-time descriptor,rec-,transfer,vibrotactile feedback,virtual realty and skills},
pages = {30--39},
title = {{Real-Time Gesture Recognition , Evaluation and Feed- Forward Correction of a Multimodal Tai-Chi Platform}},
year = {2008}
}
@article{Alankus2005,
abstract = {In this paper, we present a technique to automatically synthesize dancing motions for arbitrary songs with dance beats. Our technique is based on analysing a musical tune (can be a song or melody) and synthesizing a motion for the virtual character where the character's movement synchronizes to the musical beats. In order to analyse beats of the tune, we developed a fast algorithm. Our motion synthesis algorithm analyses library of stock motions and generates new sequences of movements that were not described in the library. We show that our motion synthesis algorithm is better than previous dance generation techniques. We also present two algorithms to synchronize dance moves and musical beats: a fast greedy algorithm, and a genetic algorithm. Our experimental results show that we can generate new sequences of dance figures in which the dancer reacts to music and dances in synchronization with the music.},
author = {Alankus, Gazihan and {Alphan Bayazit}, A. and {Burchan Bayazit}, O.},
doi = {10.1002/cav.99},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alankus, Alphan Bayazit, Burchan Bayazit - 2005 - Automated motion synthesis for dancing characters.pdf:pdf},
issn = {15464261},
journal = {Computer Animation and Virtual Worlds},
keywords = {Beat analysis,Motion analysis,Motion interpolation,Motion synthesis,Virtual choreography},
number = {3-4},
pages = {259--271},
title = {{Automated motion synthesis for dancing characters}},
volume = {16},
year = {2005}
}
@article{Han2017,
abstract = {Tai-Chi Chuan (TCC) is a famous physical exercise and well-known for being able to effectively promote physical well-being. Many people have been interested in learning TCC at the beginning, but eventually failed in mastering it due to the lack of a constantly accompanying master on the side. In this paper, we present an augmented-learning tool, called "My Tai-Chi Coaches", for learning TCC. By wearing an optical see-through Head-Mounted Display (HMD), the users can have their own private coaches-on-demand that will guide them in practicing TCC. To solve the "attention-sticking" problem, we propose the use of "redundant coaches" and high-lighting the primary coach at every instant. When the user wants to adjust his posture to mimic the coach's movement, he can simply suspend his motion, and then the drone will fly to a proper position to capture the images of the user's posture, and display them on an augmented mirror placed near by the highlighted or gazed coach. In addition to learning TCC, the proposed augmented-learning tool can also be used for learning dancing, yoga, sporting, and for rehabilitation.},
author = {Han, Ping-Hsuan and Chen, Yang-Sheng and Zhong, Yilun and Wang, Han-Lei and Hung, Yi-Ping},
doi = {10.1145/3041164.3041194},
file = {:C$\backslash$:/Users/stefa/Desktop/Master Arbeit/Literatur/tai chi coaches.pdf:pdf},
isbn = {9781450348355},
pages = {1--4},
title = {{My Tai-Chi coaches}},
year = {2017}
}
@article{Sodhi2012,
author = {Sodhi, Rajinder},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/LightGuide Projected Visualizations for Hand Movement.pdf:pdf},
isbn = {9781450310154},
pages = {179--188},
title = {{LightGuide : Projected Visualizations for Hand Movement Guidance}},
year = {2012}
}
@article{Sidharta2005,
abstract = {Cyclone Uppercut is a Virtual Reality (VR) boxing game designed to run in a six sided CAVE-like projection system such as the C6 at Iowa State University. In this game, the player assumes the role of a young boxer fighting in the final match to be the Japanese Rookie Boxer Champion. In the C6's immersive environment, the player uses his/her body to interface with the game. The game play requires the player to dodge enemy's attacks, and to execute various punches to win the game. In this paper we discuss the challenges of creating interfaces to simulate a sport like boxing for an immersive space. We also discuss the philosophy that influences the game play, and followed by the detail of implementation. We argue that the unique design of the game make it an experience that can only be experienced in a fully immersive environment like the C6.},
author = {Sidharta, Ronald and Cruz-Neira, Carolina},
doi = {10.1145/1178477.1178549},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sidharta, Cruz-Neira - 2005 - Cyclone Uppercut, a boxing game for an immersive environment.pdf:pdf},
isbn = {1595931104},
journal = {Ace 2005},
number = {January 2005},
pages = {363--364},
title = {{Cyclone Uppercut, a boxing game for an immersive environment}},
url = {http://delivery.acm.org/10.1145/1180000/1178549/p363-sidharta.pdf?ip=128.243.2.144{\&}id=1178549{\&}acc=ACTIVE SERVICE{\&}key=BF07A2EE685417C5.9530128DD756F5CF.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=867869100{\&}CFTOKEN=57616277{\&}{\_}{\_}acm{\_}{\_}=1494505749{\_}e837218be5a7a142899},
year = {2005}
}
@article{Hachimura2004,
abstract = { The mixed reality technology, with which scenes of the real world and the virtual world generated by CG are merged in real time, has been drawing considerable attention in the fields of entertainment, manufacturing, and so on. This work presents a prototype support system for dance training and education with the mixed reality technology and motion capture. Several functions which are thought to be significant for dance education, training and learning are devised and evaluated. Some user interface functions for this kind of new interactive systems are also proposed.},
author = {Hachimura, K. and Kato, H. and Tamura, H.},
doi = {10.1109/ROMAN.2004.1374759},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hachimura, Kato, Tamura - 2004 - A prototype dance training support system with motion capture and mixed reality technologies.pdf:pdf},
isbn = {0-7803-8570-5},
issn = {0873-7215},
journal = {RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759)},
pages = {217--222},
pmid = {17308627},
title = {{A prototype dance training support system with motion capture and mixed reality technologies}},
url = {http://ieeexplore.ieee.org/document/1374759/},
year = {2004}
}
@article{Bakogianni2007,
abstract = {The WebDANCE project experimented with 3D animation and Webtechnologies, and created a web-learning environment and associated lessons fortraditional dance e-learning. Two dances Karsilamas fromGreece and Valentine Morris from England were used in order to test the approach in two secondary schools.Experience from the WebDANCE project has shown that (a) the sameconceptualization schema can be used to document different European traditional dances, (b) Web3D can be used to create attractive and functionaldance resources, (c) there is a great interest from teachers / trainers in formal and informal educational settings that would like to use theWebDance platform and, (d) there is a great interest from content providers (traditional dance experts) to use the platform in order to documenttraditional dances and create teaching resources.},
author = {Bakogianni, Sophia and Kavakli, Evangelia and Karkou, Vicky and Tsakogianni, Maroussa},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bakogianni et al. - 2007 - Teaching traditional dance using e-learning tools Experience from the WebDance project.pdf:pdf},
journal = {Proceedings DVD of the 21st World Congress on Dance Research},
number = {May 2014},
pages = {1--16},
title = {{Teaching traditional dance using e-learning tools: Experience from the WebDance project}},
year = {2007}
}
@article{Yan2015,
author = {Yan, Shuo and Li, Hongsong},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/OutsideMe Augmenting Dancer's.pdf:pdf},
isbn = {9781450331463},
pages = {965--970},
title = {{OutsideMe : Augmenting Dancer ' s External Self-Image by Using A Mixed Reality System}},
year = {2015}
}
@article{Yang2006,
abstract = {We present a study of collaborative dancing between remote dancers in a tele-immersive environment which features 3D full and real body capturing, wide field of view, multi-display 3D rendering, and attachment free participant. We invite two professional dancers to perform collaborative dancing in the environment. The coordination requires one dancer to take the lead while the other follows by appropriate movement. Throughout the experiment, the dancers are dancing at various motion rates to evaluate how well the collaborative dancing is supported with the current technical boundary. Our important findings indicate that 1) tele-immersive environments have strong potential impact on the concept of choreography and communication of live dance performance, 2) the presence of multi-view display, real body 3D rendering, audio channel, and less intrusiveness greatly enhances the immersive and dancing experience, and 3) the level of synchronization achieved by the dancers is higher than that expected from the video rate.},
author = {Yang, Zhenyu and Yu, Bin and Wu, Wanmin and Diankov, Ross and Bajscy, Ruzena},
doi = {10.1038/nature08600},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2006 - Collaborative dancing in tele-immersive environment.pdf:pdf},
isbn = {1595934472},
issn = {00280836},
journal = {Proceedings of the 14th annual ACM international conference on Multimedia  - MULTIMEDIA '06},
keywords = {3d tele-immersive environment,collaboration,dance},
pages = {723},
title = {{Collaborative dancing in tele-immersive environment}},
url = {http://portal.acm.org/citation.cfm?doid=1180639.1180793},
year = {2006}
}
@article{Wang2001,
abstract = {Dynamic viewpoint tethering is an innovative display technique which has been proposed to support effective navigation in large-scale virtual environments, by integrating information from different frames of reference. The present study examines the effect of dynamic viewpoint tethering on performance, with respect to both local guidance and global awareness measures, in comparison with three conventional display formats: egocentric, exocentric and rigidly tethered displays. Participants were instructed to control an aircraft-shaped cursor navigating in a virtual tunnel and to answer questions about the environment. The results confirmed that global awareness performance decreased with increased egocentricity in the display frame of reference. The two tethered displays (dynamic and rigid) supported the best local guidance performance. No significant performance differences were found between the two tethered displays.},
author = {Wang, Wenbi and Milgram, Paul},
doi = {10.1177/154193120104502702},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Milgram - 2001 - Dynamic Viewpoint Tethering for Navigation in Large-scale Virtual Environments.pdf:pdf},
issn = {1071-1813},
journal = {Human Factors},
keywords = {awareness,dynamically tethered displays,navigation,virtual cameras,virtual environments},
pages = {1862--1866},
title = {{Dynamic Viewpoint Tethering for Navigation in Large-scale Virtual Environments}},
year = {2001}
}
@article{Yoshimura2005,
author = {Yoshimura, Mitsu and Mine, Norio and Kai, Tamiko and Isao, Yoshimura},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mine, Kai - Unknown - Quantification of Characteristic Features of Japanese Dance for.pdf:pdf},
isbn = {0780372220},
pages = {188--193},
title = {{Quantification of Characteristic Features of Japanese Dance for}},
year = {2005}
}
@article{Rajanna2016,
author = {Rajanna, Vijay and Vo, Patrick and Barth, Jerry and Mjelde, Matthew and Grey, Trevor and Oduola, Cassandra and Hammond, Tracy},
doi = {10.1007/s10916-015-0391-3},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/KinoHaptics An Automated, Wearable, Haptic Assisted,.pdf:pdf},
title = {{KinoHaptics : An Automated , Wearable , Haptic Assisted , Physio-therapeutic System for Post-surgery Rehabilitation and Self-care}},
year = {2016}
}
@article{Piumsomboon2017,
abstract = {Figure 1: a) A third-person point of view showing a virtual collaboration between two users in a reconstructed environment, b) System Setup with an AR user on the left in a real environment and VR on the right in a virtual reconstructed environment from the AR side (bottom), AR and VR users were looking at the same block. As they gazed longer together, the color of the block turned red showing that it was not the correct block that they were looking for (top). ABSTRACT We present CoVAR, a novel remote collaborative system combining Augmented Reality (AR), Virtual Reality (VR) and natural communication cues to create new types of collaboration. AR user can capture and share their local environment with a remote user in VR to collaborate on spatial tasks in shared space. COVAR supports various interaction methods to enrich collaboration, including gestures , head gaze, and eye gaze input, and provides virtual cues to improve awareness of a remote collaborator. We also demonstrate collaborative enhancements in VR user's body scaling and snapping to AR perspective. CCS CONCEPTS • Human-centered computing → Mixed / augmented reality;},
author = {Piumsomboon, Thammathip and Lee, Youngho and Lee, Gun and Billinghurst, Mark},
doi = {10.1145/3132818.3132822},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piumsomboon et al. - 2017 - CoVAR a collaborative virtual and augmented reality system for remote collaboration.pdf:pdf},
isbn = {9781450354042},
journal = {SIGGRAPH Asia 2017 Emerging Technologies on - SA '17},
keywords = {augmented reality,remote collaboration,virtual reality},
pages = {1--2},
title = {{CoVAR a collaborative virtual and augmented reality system for remote collaboration}},
url = {http://dl.acm.org/citation.cfm?doid=3132818.3132822},
year = {2017}
}
@article{Magnenat-thalmann2008,
abstract = {Preserving the knowledge of previous generations and passing it to new generations is challenging. This process is usually based on an educational system or in any other kind of face-to-face tradition. However, developing countries usually face a lack of well educated people so that this process is hindered. This is even more problematic for countries having recently struggled through times of war. Hence, we apply a community-centered approach to capturing expert knowledge in non-linear digital stories and repurposing it in the shape of educational games. In particular, we support the vocational training of local employees within a cultural heritage community that aims at preserving Bamiyan Valley in Afghanistan.},
archivePrefix = {arXiv},
arxivId = {9780201398298},
author = {Magnenat-thalmann, Nadia and Protopsaltou, Dimitrios and Kavakli, Evangelia},
doi = {10.1007/978-3-540-85033-5},
eprint = {9780201398298},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Magnenat-thalmann, Protopsaltou, Kavakli - 2008 - Advances in Web Based Learning - ICWL 2008.pdf:pdf},
isbn = {978-3-540-85032-8},
issn = {03029743},
number = {May 2014},
pmid = {4520227},
title = {{Advances in Web Based Learning - ICWL 2008}},
url = {http://link.springer.com/10.1007/978-3-540-85033-5},
volume = {5145},
year = {2008}
}
@article{Anonymous2016,
author = {Anonymous, Leave Authors},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/max und ullis chi paper.pdf:pdf},
isbn = {1234567245},
title = {{Guidance , Feedback , and Reflection : Framing the Design Space for Computer-Based Support of Motor Learning}},
year = {2016}
}
@article{Soga2009,
abstract = {We have developed an automatic composition system for contemporary dance by using 3D motion clips. Our goal is to develop some useful tools in dance education such as creation-support system for teachers and self-study system for students. Our approach is not creating natural connection but creating conceptual sequences using basic motions of contemporary dance. We present an experiment to assess whether sequences randomly selected would be appropriate for contemporary dance training and to determine some effective elements to integrate into the algorithm. As a result of the experiment, our proposed system was found to be useful and helpful for dance training.},
author = {Soga, Asako and Umino, Bin and Hirayama, Motoko},
doi = {10.1109/CW.2009.37},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Soga, Umino, Hirayama - 2009 - Automatic composition for contemporary dance using 3D motion clips Experiment on dance training and syste.pdf:pdf},
isbn = {9780769537917},
journal = {2009 International Conference on CyberWorlds, CW '09},
pages = {171--176},
pmid = {70648030},
title = {{Automatic composition for contemporary dance using 3D motion clips: Experiment on dance training and system evaluation}},
year = {2009}
}
@article{Kojima,
author = {Kojima, Taihei and Hiyama, Atsushi and Miura, Takahiro and Hirose, Michitaka},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/Training Archived Physical Skill through Immersive.pdf:pdf},
keywords = {augmented reality,head mounted display,immersive virtual environment,skill transfer},
pages = {51--58},
title = {{Training Archived Physical Skill through Immersive}}
}
@article{GangQian2005,
abstract = {In this paper, we report a real-time gesture driven interactive system with multimodal feedback for performing arts, especially dance. The system consists of two major parts: a gesture recognition engine and a multimodal feedback engine. The gesture recognition engine provides real-time recognition of the performer's gesture based on the 3D marker coordinates from a marker-based motion capture system. According to the recognition results, the multimodal feedback engine produces associated visual and audio feedback to the performer. This interactive system is simple to implement and robust to errors in 3D marker data. Satisfactory interactive dance performances have been successfully created and presented using the reported system.},
author = {{Gang Qian} and {Feng Guo} and Ingalls, T. and Olson, L. and James, J. and Rikakis, T.},
doi = {10.1109/icme.2004.1394550},
file = {:C$\backslash$:/Users/stefa/Desktop/Master Arbeit/Literatur/A{\_}gesture-driven{\_}multimodal{\_}interactive{\_}dance{\_}syst.pdf:pdf},
isbn = {0780386035},
number = {January},
pages = {1579--1582},
title = {{A gesture-driven multimodal interactive dance system}},
year = {2005}
}
@article{Kim2014,
author = {Kim, Gerard Jounghyun},
doi = {10.1162/105474602317473240},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/Implementation{\_}and{\_}Evaluation{\_}of{\_}Just{\_}Follow{\_}Me{\_}An.pdf:pdf},
isbn = {1054746023174},
number = {June 2002},
title = {{Implementation and Evaluation of “ Just Follow Me ”: An Immersive , VR-Based , Motion-Training System}},
year = {2014}
}
@book{Schmidt2011,
abstract = {Most of us have marveled at one time or another about how highly skilled performers in industry, sport, music, or dance seem to make their actions appear so simple and easy, performed with incredible efficiency, smoothness, style, and grace. Like the first three editions (Schmidt, 1982,1988; Schmidt {\&} Lee, 1999), the fourth edition of Motor Control and Learning: A Behavioral Emphasis was written for those who would like to understand how it is that these performers can achieve such artistry while we, as beginners in a similar task, are clumsy, inept, and unskilled. This book was written particularly as a textbook for university or college undergraduate and graduate students taking courses in human performance or motor learning, primarily in fields such as kinesiology or psychology. Students in other fields such as the neurosciences, physical and occupational therapy, biomedical or industrial engineering, and human factors/ergonomics will also find many of the concepts contained here to be of interest, as movement behavior is a part of all of them. And for those who are (or are becoming) practitioners in these fields, the principles of motor behavior outlined here should provide a solid basis for tasks such as designing human-machine systems, developing training programs in sport or industry, or teaching progressions in dance or music. The book is divided into three parts. Part I provides an introduction to research and fundamental concepts that are important to understanding motor behavior; Part II deals with motor control; and Part III deals with the acquisition of skill, or motor learning. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
author = {Schmidt, Richard and Lee, Tim},
isbn = {0736079610},
title = {{Motor Control and Learning: A Behavioral Approach}},
year = {2011}
}
@article{Hachimura2005,
abstract = {The final goal of this research is to extract characteristic poses as well as highlight parts from data of dancing movement obtained by motion capturing technique. For this, the theory of Laban movement analysis (LMA) has been applied, and the physical feature values corresponding to the LMA components are defined. By observing the change over time of these feature values, body movements corresponding to the LMA components are extracted. In this paper we mainly focus on effort and shape components of LMA. Results have been compared with those which have been extracted by a LMA specialist.},
author = {Hachimura, Kozaburo and Takashina, Katsumi and Yoshimura, Mitsu},
doi = {10.1109/ROMAN.2005.1513794},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hachimura, Takashina, Yoshimura - 2005 - Analysis and evaluation of dancing movement based on LMA.pdf:pdf},
isbn = {0780392752},
issn = {1944-9445},
journal = {Proceedings - IEEE International Workshop on Robot and Human Interactive Communication},
pages = {294--299},
title = {{Analysis and evaluation of dancing movement based on LMA}},
volume = {2005},
year = {2005}
}
@article{Brockhoeft2016,
abstract = {" Like the overlap in a Venn diagram, shared kinesthetic and intellectual constructs from the field of dance and the field of technology will reinforce and enhance one another, resulting in an ultimately deepened experience for both viewer and performer. " -Alyssa Schoeneman Abstract With the rise of the digital age, dancers and choreog-raphers started looking for new ways to connect with younger audiences who were left disengaged from tra-ditional dance productions. This led to the growing pop-ularity of multimedia performances where digitally pro-jected spaces appear to be influenced by dancers' move-ments. Unfortunately current approaches, such as re-liance on pre-rendered videos, merely create the illusion of interaction with dancers, when in fact the dancers are actually closely synchronized with the multimedia display to create the illusion. This calls for unprece-dented accuracy of movement and timing on the part of the dancers, which increases cost and rehearsal time, as well as greatly limits the dancers' creative expression. We propose the first truly interactive solution for inte-grating digital spaces into dance performance: ViFlow. Our approach is simple, cost effective, and fully interac-tive in real-time, allowing the dancers to retain full free-dom of movement and creative expression. In addition, our system eliminates reliance on a technical expert. A movement-based language enables choreographers to directly interact with ViFlow, empowering them to inde-pendently create fully interactive, live augmented real-ity productions.},
author = {Brockhoeft, Taylor and Petuch, Jennifer and Bach, James and Djerekarov, Emil and Ackerman, Margareta and Tyson, Gary},
doi = {10.1007/BF01533262},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brockhoeft et al. - 2016 - Interactive Augmented Reality for Dance.pdf:pdf},
isbn = {978-274-669-155-1},
journal = {Proceedings of the Seventh International Conference on Computational Creativity (ICCC 2016)},
number = {June},
pages = {396--403},
title = {{Interactive Augmented Reality for Dance}},
url = {http://www.computationalcreativity.net/iccc2016/wp-content/uploads/2016/01/Interactive-Augmented-Reality-for-Dance.pdf},
year = {2016}
}
@article{Kwon2005,
abstract = {We present a new framework to build motion training systems using machine learning techniques. The goal of our approach is the design of a training method based on the combination of body and visual sensors. We introduce the concept of a Motion Chunk to analyze human motion and construct a motion data model in real-time. The system provides motion detection and evaluation and visual feedback generation. We discuss the results of user tests regarding the system efficiency in martial art training. With our system, trainers can generate motion training videos and practice complex motions precisely evaluated by a computer.},
author = {Kwon, Doo Young and Gross, Markus},
doi = {10.1145/1178477.1178490},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kwon, Gross - 2005 - Combining body sensors and visual sensors for motion training.pdf:pdf},
isbn = {1595931104},
journal = {Proceedings of the 2005 ACM SIGCHI International Conference on Advances in computer entertainment technology  - ACE '05},
keywords = {body sensor,mo-,motion analysis,motion training system,tion chunk,visual feedback,visual sensor},
pages = {94--101},
title = {{Combining body sensors and visual sensors for motion training}},
url = {http://portal.acm.org/citation.cfm?doid=1178477.1178490},
year = {2005}
}
@article{Kim2003,
author = {Kim, Tae-Hoon and Park, Sang Il and Shin, Sung Yong},
doi = {10.1145/882262.882283},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Park, Shin - 2003 - Rhythmic-motion synthesis based on motion-beat analysis.pdf:pdf},
isbn = {1581137095},
issn = {07300301},
journal = {{\{}ACM{\}} Trans. Graph.},
keywords = {beat analysis,motion blending,motion signal pro-,motion syn-,motion synchronization,motion transition,thesis},
number = {3},
pages = {392--401},
title = {{Rhythmic-motion synthesis based on motion-beat analysis}},
volume = {22},
year = {2003}
}
@article{Murray-Reichel2004,
abstract = {In this paper, we report a real-time geshire driven interactive system with multimodal feedback for performing arts, especially dance. The system consists of two major paris: a gesture recopifion engine and a multimodal feedhack engine. The gesture recognition engine provides real-time recognition of the performer's geshrre based on the 3 0 marker coordinates from a marker-based motion capture system. According to the recognition results, the muliimodal feedhack engine produces associated visual and audio feedback to the peformer. This interactive system is simple to implement and robust fa errors in 3 0 marker daia. Satisjactory interaciive dance performances have been successfilly created andpresented using the reported system.},
author = {Murray-Reichel, Desmond},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murray-Reichel - 2004 - I'll reboot the primary EXE program, that should port the SMS alarm!.pdf:pdf},
isbn = {0780386035},
title = {{I'll reboot the primary EXE program, that should port the SMS alarm!}},
year = {2004}
}
@article{Liu2006,
abstract = {Traditionally, when people want to learn martial arts, they have to go to training clubs and learn under the coach together with the other students. To reduce the tuition fees, there are usually a lot of students under a single coach and hence, it is difficult for the students to get enough suggestions in the class. It would be far easier if the students could practice themselves at home and ask for suggestions from a virtual coach in the computer. Occasionally, in case they find difficulties going to the next step, they may then approach the real coach for suggestions and training. In this paper, we propose such a training system based on the motion capture system. The system automatically analyzes the motions of the player and gives suggestions. The students can also view the martial art techniques stored in the system or their own techniques captured by the motion capture system from various points of view in order to gain objective ideas of the techniques.},
author = {Liu, W and Li, Q and Lau, R W H and Komura, Taku and Lam, Beta and Lau, Rynson W H and Leung, Howard},
doi = {10.1007/11925293_22},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2006 - e-Learning Martial Arts.pdf:pdf},
isbn = {3-540-49027-2},
issn = {0302-9743},
journal = {Lncs},
keywords = {automatic motion analysis,martial arts,motion,web-based learning},
pages = {239--248},
title = {{e-Learning Martial Arts}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2F11925293{\_}22.pdf},
volume = {4181},
year = {2006}
}
@article{Choensawat2015,
abstract = {This paper presents a computer-aided tool for automatically generating Labanotation scores from motion capture data named GenLaban. GenLaban can be implemented with a low-cost equipment but an efficient method that allows users converting body motions to scores. The key components of GenLaban are the analysis of body motions, the quantization of body postures and the determination of body parts carrying the body weight. All the processes are under supervision of a Labanotation expert to ensure the notation meaning correctly as the use for the dance composition. The experiments showed that for dancers, dance instructors and choreographers, GenLaban is a potential tool for notating dance movements into Labanotation scores enabling them to be accurately interpreted. At present the system can handle a subset of Labanotation covering many of the fundamental movements. However, Labanotation is rich in symbols and new symbols are continually being introduced and will be incorporated in the GenLaban tool as time permits. {\textcopyright} Springer Science+Business Media New York 2014.},
author = {Choensawat, Worawat and Nakamura, Minako and Hachimura, Kozaburo},
doi = {10.1007/s11042-014-2209-6},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/Literatur/GenLaban-2014.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Dance notation,LabanEditor,Labanotation data,Movement Analysis},
number = {23},
pages = {10823--10846},
title = {{GenLaban: A tool for generating Labanotation from motion capture data}},
volume = {74},
year = {2015}
}
@article{Chua,
author = {Chua, Philo Tan and Schaaf, Russ},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/Training for Physical Tasks in Virtual Environments Tai Chi.pdf:pdf},
title = {{Training for Physical Tasks in Virtual Environments : Tai Chi}}
}
@article{Kim2016,
abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}p{\textgreater}Physiologic turnover of interstitial collagen is mediated by a sequential pathway in which collagen is fragmented by pericellular collagenases, endocytosed by collagen receptors, and routed to lysosomes for degradation by cathepsins. Here, we use intravital microscopy to investigate if malignant tumors, which are characterized by high rates of extracellular matrix turnover, utilize a similar collagen degradation pathway. Tumors of epithelial, mesenchymal, or neural crest origin all display vigorous endocytic collagen degradation. The cells engaged in this process are identified as tumor-associated macrophage (TAM)-like cells that degrade collagen in a mannose receptor-dependent manner. Accordingly, mannose-receptor-deficient mice display increased intratumoral collagen. Whole-transcriptome profiling uncovers a distinct extracellular matrix-catabolic signature of these collagen-degrading TAMs. Lineage-ablation studies reveal that collagen-degrading TAMs originate from circulating CCR2+ monocytes. This study identifies a function of TAMs in altering the tumor microenvironment through endocytic collagen turnover and establishes macrophages as centrally engaged in tumor-associated collagen degradation.{\textless}/p{\textgreater}},
author = {Kim, Youngsun and Hong, Seokjun and Kim, Gerard J.},
doi = {10.1145/2993369.2996301},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Hong, Kim - 2016 - Augmented reality based remote coaching system.pdf:pdf},
isbn = {9781450344913},
issn = {22111247},
journal = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology - VRST '16},
keywords = {augmented reality,concepts,feedback,human-centered computing,interaction,mixed,multimodal,paradigms,pre-attentive recognition,tele-coaching},
pages = {311--312},
title = {{Augmented reality based remote coaching system}},
url = {http://dl.acm.org/citation.cfm?doid=2993369.2996301},
year = {2016}
}
@article{Chan2007,
author = {Chan, Jacky and Leung, Howard and Tang, Kai Tai and Komura, Taku},
doi = {10.4108/ICST.IMMERSCOM2007.2102},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chan et al. - 2007 - Immersive Performance Training Tools Using Motion Capture Technology.pdf:pdf},
isbn = {978-963-9799-06-6},
journal = {Proceedings of the ImmersCom},
keywords = {Immersive VR application, Human Computer Interacti},
title = {{Immersive Performance Training Tools Using Motion Capture Technology}},
url = {http://eudl.eu/doi/10.4108/ICST.IMMERSCOM2007.2102},
year = {2007}
}
@article{Pfeil2017,
author = {Pfeil, Ulrike and D{\"{u}}rr, Maximilian},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/Einsatzm{\"{o}}glichkeiten von Mixed Reality zur Unterst{\"{u}}tzung.pdf:pdf},
keywords = {kinaesthetics,mixed reality,motorisches lernen},
pages = {11--22},
title = {{Einsatzm{\"{o}}glichkeiten von Mixed Reality zur Unterst{\"{u}}tzung von motorischem Lernen}},
year = {2017}
}
@article{Davcev2003,
abstract = {We emphasize the generation of an augmented reality environment of a single dancer based on analysis of dance annotations. We introduce a new Web3D-based interactive technique for dance animation needed for educational purposes. This approach offers new possibilities for interactive dance step observation, slow movements of fast steps, different angles of view and several functions as a support of high interactivity between the user and the 3D dancer. The benefits of the approach have been estimated by dance experts from the Macedonian Folklore Institute, and the "Mirche Acev" folk ensemble.},
author = {Davcev, D. and Trajkovic, V. and Kalajdziski, S. and Celakoski, S.},
doi = {10.1109/ITRE.2003.1270600},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davcev et al. - 2003 - Augmented reality environment for dance learning.pdf:pdf},
isbn = {0780377249},
journal = {Proceedings, ITRE 2003 - International Conference on Information Technology: Research and Education},
pages = {189--193},
title = {{Augmented reality environment for dance learning}},
year = {2003}
}
@article{Magnenat-thalmann1990,
author = {Magnenat-thalmann, Nadia and Joslin, Chris},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Magnenat-thalmann, Joslin - 1990 - Learning how to Dance on the Internet.pdf:pdf},
keywords = {7,advanced interactivity,called dvs,commercial system,dance,humans,internet,networked virtual environments,teaching,virtual,was developed as a},
number = {May},
title = {{Learning how to Dance on the Internet}},
year = {1990}
}
@article{Chan2011,
author = {Chan, Jacky C P and Leung, Howard and Tang, Jeff K T and Komura, Taku},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/A Virtual Reality Dance Training System.pdf:pdf},
isbn = {2009120167},
number = {2},
pages = {187--195},
title = {{A Virtual Reality Dance Training System Using Motion Capture Technology}},
volume = {4},
year = {2011}
}
@article{Anderson2013,
author = {Anderson, Fraser and Grossman, Tovi and Matejka, Justin and Fitzmaurice, George},
file = {:C$\backslash$:/Users/stefa/OneDrive/Master Arbeit/Literatur/YouMove Enhancing Movement Training.pdf:pdf},
isbn = {9781450322683},
pages = {311--320},
title = {{YouMove : Enhancing Movement Training with an Augmented Reality Mirror}},
year = {2013}
}
@article{DeSilva2004,
abstract = {One of the challenging issues in affective computing is to give a machine the ability to recognize the mood of a person. Efforts in that direction have mainly focused on facial and oral cues. Gestures have been recently considered as well, but with less success. Our aim is to fill this gap by identifying and measuring the saliency of posture features that play a role in affective expression. As a case study, we collected affective gestures from human subjects using a motion capture system. We first described these gestures with spatial features, as suggested in studies on dance. Through standard statistical techniques, we verified that there was a statistically significant correlation between the emotion intended by the acting subjects, and the emotion perceived by the observers. We used Discriminant Analysis to build affective posture predictive models and to measure the saliency of the proposed set of posture features in discriminating between 4 basic emotional states: angry, fear, happy, and sad. An information theoretic characterization of the models shows that the set of features discriminates well between emotions, and also that the models built over-perform the human observers.},
author = {{De Silva}, P. Ravindra and Bianchi-Berthouze, Nadia},
doi = {10.1002/cav.29},
file = {:C$\backslash$:/Users/stefa/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/De Silva, Bianchi-Berthouze - 2004 - Modeling human affective postures An information theoretic characterization of posture features.pdf:pdf},
isbn = {0780344634},
issn = {15464261},
journal = {Computer Animation and Virtual Worlds},
number = {3-4},
pages = {269--276},
title = {{Modeling human affective postures: An information theoretic characterization of posture features}},
volume = {15},
year = {2004}
}
@misc{,
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/new papers.txt:txt},
title = {new papers}
}
@article{,
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/physio1.pdf:pdf},
isbn = {9781450324748},
title = {{Physio @ Home : Design Explorations to Support Movement Guidance}}
}
@article{Tang2015,
author = {Tang, Richard and Tang, Anthony and Scott, Xing-dong Yang and Jorge, Joaquim},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/physio2.pdf:pdf},
isbn = {9781450331456},
title = {{Physio @ Home : Exploring visual guidance and feedback techniques for physiotherapy exercises}},
year = {2015}
}
@article{Medeiros,
author = {Medeiros, Daniel and T{\'{e}}cnico, Inesc-id Lisboa and Ars{\'{e}}nio, Artur and Jorge, Joaquim and T{\'{e}}cnico, Inesc-id Lisboa},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/sleeveAR.pdf:pdf},
isbn = {9781450341370},
title = {{SleeveAR : Augmented Reality for Rehabilitation using Realtime Feedback}}
}
@misc{,
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/stroke.pdf:pdf},
title = {stroke.pdf}
}
@article{Katzakis2017,
author = {Katzakis, Nicholas and Tong, Jonathan and Chen, Lihan and R{\"{o}}der, Brigitte and Steinicke, Frank},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/styloHandifact.pdf:pdf},
isbn = {9781450354868},
keywords = {Visual, Haptic, Integration, Multisensory, Sensory, Perception, Posture Training, Wearable, Interface,acm reference format,haptic,integration,interface,multisensory,perception,pos-,sensory,ture training,visual,wearable},
pages = {58--67},
title = {{Stylo and Handifact : Modulating Haptic Perception through Visualizations for Posture Training in Augmented Reality}},
year = {2017}
}
@article{Spelmezan2009,
author = {Spelmezan, Daniel and Borchers, Jan},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/tactilemotion.pdf:pdf},
isbn = {9781605582467},
keywords = {vibrotactile feedback, real-time instructions, physical activities, sports training, motor skills},
title = {{Tactile Motion Instructions For Physical Activities}},
year = {2009}
}
@article{Lieberman2007,
author = {Lieberman, Jeff and Breazeal, Cynthia},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/tikl.pdf:pdf},
number = {5},
pages = {919--926},
title = {{TIKL : Development of a Wearable Vibrotactile Feedback Suit for Improved Human Motor Learning}},
volume = {23},
year = {2007}
}
@article{Linden2011,
author = {Linden, Janet Van Der and Johnson, Rose and Bird, Jon and Rogers, Yvonne and Schoonderwaldt, Erwin},
doi = {10.1145/1978942.1979017},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/vanderLindenCHI2011buzzingtoplay.pdf:pdf},
isbn = {9781450302678},
number = {May},
title = {{Buzzing to Play : Lessons Learned From an In the Wild Study of Real-time Vibrotactile Feedback}},
year = {2011}
}
@article{Author,
author = {Author, Anonymous},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/authordraft{\_}EGuide.pdf:pdf},
isbn = {9781450399999},
keywords = {a,common resources for their,e,ever,focus between an external,g,how-,important for various activities,mid-air arm movements are,self-directed practice require prac-,source,titioners to divide their},
title = {{EGuide : Investigating different Visual Appearances and pu b No lis t f he or d w di o str rk ib ing ut d io ra n . ft Un Un pu b No lis t f he or d w di o str rk ib ing ut d io ra n . ft}}
}
@article{Kyan2015,
author = {Kyan, Matthew},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/ballet.pdf:pdf},
number = {2},
title = {{An Approach to Ballet Dance Training through MS Kinect and Visualization in a CAVE Virtual Reality Environment}},
volume = {6},
year = {2015}
}
@article{Covaci2014,
author = {Covaci, Alexandra},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/basketball.pdf:pdf},
isbn = {9781450332538},
keywords = {bas-,design challenges that need,immersive room,ketball training,perception of distance in,performance,pro-,technological and software facilities,there are still many,to be tackled to,to im-,vide users with efficient,visual feedback,vr},
pages = {55--64},
title = {{Third Person View And Guidance For More Natural Motor Behaviour In Immersive Basketball Playing}},
year = {2014}
}
@article{,
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/cscw.pdf:pdf},
number = {l},
title = {{38 January 1991/Vol.34, No.l/COMMUNiCATiONS OF THE ACM}},
volume = {34},
year = {1991}
}
@article{Scavo2015,
author = {Scavo, Giuseppe and Wild, Fridolin and Scott, Peter J},
doi = {10.13140/RG.2.1.5069.1682},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/GhostHands.pdf:pdf},
number = {July},
title = {{The GhostHands UX : telementoring with hands-on augmented reality instruction}},
year = {2015}
}
@article{Chinthammit2014,
author = {Chinthammit, Winyu and Merritt, Troy and Pedersen, Scott and Williams, Andrew and Visentin, Denis and Rowe, Robert and Furness, Thomas},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/ghostman.pdf:pdf},
title = {{Ghostman : Augmented Reality Application for Telerehabilitation and Remote Instruction of a Novel Motor Skill}},
volume = {2014},
year = {2014}
}
@article{Baek2001,
author = {Baek, Seongmin and Lee, Seungyong and Kim, Gerard Jounghyun},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/motionEval.pdf:pdf},
number = {3},
title = {{Motion Evaluation for VR-based Motion Training}},
volume = {20},
year = {2001}
}
@article{Velloso2013,
author = {Velloso, Eduardo and Gellersen, Hans},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/motionMA.pdf:pdf},
isbn = {9781450318990},
keywords = {Activity Assessment,Learning by Demonstration,Motion Modelling,Real-Time User Feedback,Weight Lifting},
pages = {1309--1318},
title = {{MotionMA : Motion Modelling and Analysis by Demonstration}},
year = {2013}
}
@article{Portillo-rodriguez2008a,
author = {Portillo-rodriguez, Otniel and Sandoval-gonzalez, Oscar O and Ruffaldi, Emanuele},
doi = {10.1007/978-3-540-87883-4},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/multimodalTaiChi.pdf:pdf},
isbn = {9783540878834},
keywords = {audio-position,feedback,multimodal interfaces,real-time 3d time-independent gesture,real-time descriptor,recognition,transfer,vibrotactile feedback,virtual realty and skills},
number = {May 2014},
title = {{Real-Time Gesture Recognition , Evaluation and Feed-Forward Correction of a Multimodal Tai-Chi Platform Real-Time Gesture Recognition , Evaluation and Feed- Forward Correction of a Multimodal Tai-Chi Platform}},
year = {2008}
}
@article{Johnson2010,
author = {Johnson, Rose and Linden, Janet Van Der},
doi = {10.1145/1753846.1754004},
file = {:C$\backslash$:/Users/stefa/Dropbox/Master Arbeit/new papers/musicJacket.pdf:pdf},
number = {May 2014},
title = {{Open Research Online The Open University ' s repository of research publications MusicJacket : the efficacy of real-time vibrotactile feed- back for learning to play the violin Conference Item}},
year = {2010}
}
